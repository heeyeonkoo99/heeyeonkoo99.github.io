---
title: "[딥러닝을 이용한 자연어 처리 입문] 요약정리"
date: 2021-07-16 08:26:28 -0400
categories: data-analysis
url: /data-analysis/
---

> ## 시작에 앞서
이 책은 전통적인 자연어 처리방법과 인공 신경망에 대해서 다룬다. 또한 텐서플로우의 케라스 API를 주로 사용한다. 1챕터부터 13챕터를 기본과정으로 하고 14챕터부터 19챕터를 심화과정으로 본다.

> ## 참고하여 볼것
동일 저자가 만든 PyTorch 학습 자료 : <https://wikidocs.net/book/2788>   
이 책을 위해 제작한 이미지 자료 공유 (영리적 목적 제외 자유롭게 사용) : <https://www.slideshare.net/wonjoonyoo/ss-188835227>   
딥 러닝 추천 자료 : <https://www.d2l.ai/index.html>   
NLP 참고 자료 : <http://www.phontron.com/class/nn4nlp2019/schedule.html>, <https://github.com/makcedward/nlp>   
한국어 NLP 논문 모음 : <https://github.com/papower1/Awesome-Korean-NLP-Papers>   

# 01. 자연어 처리(natural language processing)란?
자연어(natural language)란 우리가 일상 생활에서 사용하는 언어를 말한다. 자연어 처리(natural language processing)란 이러한 자연어의 의미를 분석하여 컴퓨터가 처리할 수 있도록 하는 일을 말한다. 자연어 처리는 음성 인식, 내용 요약, 번역, 사용자의 감성 분석, 텍스트 분류 작업(스팸 메일 분류, 뉴스 기사 카테고리 분류), 질의 응답 시스템, 챗봇과 같은 곳에서 사용되는 분야이다. 
컴퓨터와 인간 언어 사이의 상호작용하는 기술로 인공지능의 핵심 기능중 하나다. 

### 1) 필요 프레임워크와 라이브러리
윈도우 환경을 기준으로 두고 아나콘다(Anaconda)를 설치하는 방법과 인터넷을 통해 간편히 사용할 수 있는 실습환경인 구글의 코랩(Colab)이 소개되었다. (~~나는 이미 아나콘다가 설치되어있으므로 굳이 새로운 걸 깔지않고 그대로 진행한다.~~) 아나콘다를 설치했다면 기본적으로 Numpy, Pandas, Jupyter notebook, scikit-learn, matplotlib, seaborn, nltk 등이 이미 설치되어져 있다. 그래서 아나콘다에 포함되어있지 않은 tensorflow, keras, gensim 이 세 가지만 별도로 pip를 통해 설치하면 된다.   

✅ __텐서플로우(Tensorflow)__     
텐서플로우는 구글이 2015년에 공개한 머신 러닝 오픈소스 라이브러다. 머신 러닝과 딥 러닝을 직관적이고 손쉽게 할 수 있도록 설계되었다. 뒤의 딥 러닝을 실습을 위해서 텐서플로우를 설치해야 한다. 아나콘다 프롬프트(Anaconda Prompt) 또는 명령 프롬프트를 통해서 설치할 수 있다. 아나콘다 프롬프트를 열었다면 아나콘다 프롬프트에 아래의 커맨드를 입력하여 텐서플로우를 설치한다.
```python
> pip install tensorflow
```
<img src = "https://user-images.githubusercontent.com/68431716/125898456-1225d98c-eb13-4ae6-aa9e-d7a5eb345ed3.png" width="1000px">

✅ __케라스(Keras)__     
케라스(Keras)는 딥 러닝 프레임워크인 텐서플로우에 대한 추상화 된 API를 제공한다. 케라스는 백엔드로 텐서플로우를 사용하며, 좀 더 쉽게 딥 러닝을 사용할 수 있게 해준다. 쉽게 말해, 텐서플로우 코드를 훨씬 간단하게 작성할 수 있다. 사실 설치한 케라스를 사용할 수도 있지만, 텐서플로우에서 케라스를 사용할 수도 있다. 영어 커뮤니티에서는 순수 케라스를 keras라고 표기한다면, 텐서플로우에서 케라스 API를 사용하는 경우는 tf.keras라고 표기한다. 이 두 가지는 실제로 문법도 많은 면에서 같아서 keras 코드를 tf.keras로 변경하는 건 아주 쉽고, keras를 학습하였다면 tf.keras도 금방 익숙하게 사용할 수 있다. 케라스 개발자인 프랑소와 숄레(François Chollet)는 앞으로는 keras보다는 tf.keras를 사용할 것을 권장한다. 이 책은 keras와 tf.keras 두 가지 모두를 사용한다.   
(케라스설치할때는 텐서플로우 설치할때랑 다르게 계속해서 오류가 뜨고 설치에 실패했었다.ㅠ그 원인을 알아보니 결국 케라스가 텐서플로우에 기반하기때문에 __반드시__ 둘이 호환가능한 파일로 구성되어야 한다. 그래서 uninstall했다가 여러시도를 거듭한 결과, 설치성공했다!😁)
```python
> pip install keras
```
<img src = "https://user-images.githubusercontent.com/68431716/125938385-d866310c-718f-4070-afe6-9c204c764ee4.png" width="1000px">


✅ __젠심(Gensim)__   
젠심(Gensim)은 머신 러닝을 사용하여 토픽 모델링과 자연어 처리 등을 수행할 수 있게 해주는 오픈 소스 라이브러리다. 이 책에서도 젠심을 사용하여 토픽 모델링과 Word2Vec 등을 학습해본다.  
```python
> pip install gensim
```
<img src = "https://user-images.githubusercontent.com/68431716/125939064-e30ac44f-e75a-4045-a2f8-79d877cc01a5.png" width="1000px">

### 2) 자연어 처리를 위한 NLTK와 KoNLPy 설치하기
다음 챕터인 텍스트 전처리(Text preprocessing) 챕터에서는 전처리를 위한 이론에 대해서 학습하고, 그 이론을 바탕으로 실습을 진행하게 된다. 이번 챕터에서는 실습에 필요한 기본적인 자연어 패키지가 소개된다.  
-NLTK는 자연어 처리를 위한 파이썬 패키지로 아나콘다가 설치되었다면 기본적으로 설치되어있다. 이때 NLTK의 기능을 제대로 사용하기 위해 NLTK Data라는 여러 데이터를 추가적으로 설치해야 한다. 이를 위해서는 파이썬 코드 내에서 import nltk 이후에 nltk.download()라는 코드를 수행하여 설치한다.  
<img src = "https://user-images.githubusercontent.com/68431716/125940574-a96bdcbd-7130-45d6-acef-ca965060d69c.png" width="800px">
   
코엔엘파이(KoNLPy)는 한국어 자연어 처리를 위한 형태소 분석기 패키지다.  
<img src = "https://user-images.githubusercontent.com/68431716/125939975-0b3a6e73-7b33-4c4f-800c-d88160f94e83.png" width="800px">

💡 Pandas, Numpy and Matplotlib 실습 =>  
<https://github.com/heeyeonkoo99/Deep-learning_study/blob/master/%EC%8B%A4%EC%8A%B5/Pandas%20and%20Numpy%20and%20Matplotlib.ipynb>

# 02. 텍스트 전처리(Text preprocessing)  
자연어 처리에 있어서 텍스트 전처리는 매우 중요한 작업이다. 텍스트 전처리는 용도에 맞게 텍스트를 사전에 처리하는 작업이다. 요리에 있어서 재료를 제대로 손질하지 않으면, 요리가 제대로 되지 않는 것과 같다. 텍스트에 대해서 제대로 된 전처리를 하지 않으면 뒤에서 배울 자연어 처리 기법들이 제대로 동작하지 않다. 이 챕터에서는 텍스트를 전처리하기 위한 각종 기법에 대해서 배운다.

💡 Text preprocessing 실습 =>  
<https://github.com/heeyeonkoo99/Deep-learning_study/blob/master/%EC%8B%A4%EC%8A%B5/2_Text%20preprocessing.ipynb>


# 03. 언어 모델(Language Model)  
언어 모델(Languagel Model)이란 단어 시퀀스(문장)에 확률을 할당하는 모델을 말한다. 어떤 문장들이 있을 때, 기계가 이 문장은 적절해! 이 문장은 말이 안 돼! 라고 사람처럼 판단할 수 있다면, 기계가 자연어 처리를 정말 잘 한다고 볼 수 있다. 이게 바로 언어 모델이 하는 일이다.  
이번 챕터에서는 통계에 기반한 전통적인 언어 모델(Statistical Languagel Model, SLM)에 대해서 배운다. 통계에 기반한 언어 모델은 우리가 실제 사용하는 자연어를 근사하기에는 많은 한계가 있었고, 요즘 들어 인공 신경망이 그러한 한계를 많이 해결해주면서 통계 기반 언어 모델은 많이 사용 용도가 줄었다. 하지만 그럼에도 여전히 통계 기반 언어 모델에서 배우게 될 n-gram은 자연어 처리 분야에서 활발하게 활용되고 있으며, 통계 기반 방법론에 대한 이해는 언어 모델에 대한 전체적인 시야를 갖는 일에 도움이 된다.  


### 1) 언어 모델(Language Model)이란?  
언어 모델(Language Model, LM)은 언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당(assign)하는 모델이다. 언어 모델을 만드는 방법은 크게 __통계를 이용한 방법__ 과 __인공 신경망을 이용한 방법__ 으로 구분할수 있다. 최근에는 인공신경망을 이용한 방법(ex. GPT, BERT)이 더 좋은 성능을 보여주고 있다. 이번 챕터에서는 전통적인 통계적 언어 모델에 대해서 배운다.

=> 언어 모델: 단어 시퀀스에 확률을 할당(assign)하는 일을 하는 모델이다. 즉 언어 모델은 가장 자연스러운 단어 시퀀스를 찾아내는 모델이다. 단어 시퀀스에
확률을 할당하게 하기 위해서 가장 보편적으로 사용되는 방법은 언어 모델이 이전 단어들이 주어졌을때 다음 단어를 예측하도록 하는것이다.  
=> 언어 모델링: 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 말한다.  
*스태폰드대학에서는 언어 모델을 문법이라고 비유하기도 한다.*  
=> 검색 엔진에서의 언어 모델의 예를 생각해볼수 있다.  

### 2) 통계적 언어 모델(Statistical Language Model, SLM)   
=> 조건부 확률: <img src = "https://user-images.githubusercontent.com/68431716/126054935-b566f1ee-2047-43c3-aa42-b0e1b678ed63.png" width="900px">  
=> 문장에 대한 확률: <img src = "https://user-images.githubusercontent.com/68431716/126054938-ec7a114c-4247-4d03-b84d-67ea302e0675.png" width="900px">  
=> 카운트 기반의 접근: <img src = "https://user-images.githubusercontent.com/68431716/126054941-9051e0f0-9100-4ab0-94c3-30f6a6ee1a10.png" width="900px">  
=> 카운트 기반접근의 한계- 희소 문제: 충분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 말한다. 코퍼스에 단어 시퀀스가 없다면 그 확률이 0 또는 정의되지 않는 확률이 되어버린다.  

### 3) N-gram 언어 모델(N-gram Language Model)  
n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종이다. 다만, 앞서 배운 언어 모델과는 달리 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만
고려하는 접근 방법을 사용한다. 이때 일부 단어를 몇개 보느냐를 결정하는데 이것이 n-gram에서의 n이 가지는 의미다.   
=> SLM의 한계는 훈련 코퍼스에 확률을 계산하고 싶은 문장이나 단어가 없을수 있다는 점이다. 그리고 확률을 계산하고 싶은 문장이 길어질수록 갖고있는 코퍼스에서
그 문장이 존재하지 않을 가능성이 높다. 다시말하면 카운트 할수 없는 가능성이 높다. 그러나 참고하는 단어들을 줄이면 카운트를 할수 있는 가능성을 높일수 있다.  
예를 들어, An adorable little boy에서 is가 나올 확률보다 boy에서 is가 나올 확률이 더 높을것이다.  
=> n-gram은 n개의 연속적인 단어 나열을 의미한다. 이때 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 이를 하나의 토큰으로 간주한다.  
<img src = "https://user-images.githubusercontent.com/68431716/126055148-accfbe13-30e2-463e-ac08-25823abf185c.png" width="900px" height="100px">   
=> n-gram language nodel의 한계로는 희소 문제/ n을 선택하는 것은 trade-off문제 등이 있다.
=> 이 모델의 한계를 극복하기 위해 인공 신경망을 이용한 언어 모델이 많이 사용되고 있다.



# 04. 카운트 기반의 단어 표현(Count based word Representation)  
자연어 처리에서 텍스트를 표현하는 방법으로는 여러가지 방법이 있다. 우리가 앞서 배운 n-gram 또한 텍스트를 표현하는 방법 중 하나이다. 하지만 머신 러닝 등의 알고리즘이 적용된 본격적인 자연어 처리를 위해서는 문자를 숫자로 수치화할 필요가 있다. 그런 측면에서 앞으로 4챕터, 6챕터, 10챕터에서는 문자를 숫자로 수치화하는 방법에 대해서 배우게 된다.  

### 1) 다양한 단어의 표현 방법  
우리는 카운트 기반의 단어 표현 방법을 다음 챕터인 Bag of Words챕터에서부터 배운다.  
=> 단어의 표현 방법으론 크게 __국소 표현(Local Representation/ Discrete Representation)방법__ 과 __분산 표현(Distributed Representation/ Continuous Representation)방법__ 으로 나뉜다. 국소 표현 방법은 해당 단어 그 자체만 보고, 특정값을 맵핑하여 단어를 표현하는 방법이며, 분산 표현 방법은 그 단어를 표현하고자 주변을 참고하여 단어를 표현하는 방법이다. 예를 들어 puppy(강아지), cute(귀여운), lovely(사랑스러운)라는 단어가 있을 때 각 단어에 1번, 2번, 3번 등과 같은 숫자를 맵핑(mapping)하여 부여한다면 이는 국소 표현 방법에 해당된다. 반면, 분산 표현 방법의 예를 하나 들어보면 해당 단어를 표현하기 위해 주변 단어를 참고한다. puppy(강아지)라는 단어 근처에는 주로 cute(귀여운), lovely(사랑스러운)이라는 단어가 자주 등장하므로, puppy라는 단어는 cute, lovely한 느낌이다로 단어를 정의한다. 이렇게 되면 이 두 방법의 차이는 국소 표현 방법은 단어의 의미, 뉘앙스를 표현할 수 없지만, 분산 표현 방법은 단어의 뉘앙스를 표현할 수 있게 된다.  

 
<img src = "https://user-images.githubusercontent.com/68431716/126105832-68baff85-3d7b-4c15-a4ff-43c8b91b654f.png" width="1000px" height="400px">

이번 4챕터의 Bag of Words는 국소 표현에 속하며, 단어의 빈도수를 카운트(Count)하여 단어를 수치화하는 단어 표현 방법이다. 이 챕터에서는
BoW와 그의 확장인 DTM(=TDM)에 대해 학습하고, 이러한 빈도수 기반 단어 표현에 단어의 중요도에 따른 가중치를 줄수 있는 TF-IDF에 대해 학습한다.  

💡 Count based words 실습 =>  
<https://github.com/heeyeonkoo99/Deep-learning_study/blob/master/%EC%8B%A4%EC%8A%B5/4_Count%20based%20words.ipynb>


# 05. 벡터의 유사도(Vector Similarity)  
문서의 유사도를 구하는 일은 자연어 처리의 주요 주제 중 하나다. 기계가 계산하는 문서의 유사도의 성능은 어떤
방법으로 수치화하여 표현했는지(DTM, Word2Vec), 문서 간의 단어들의 차이를 어떤 방법(유클리드의 거리, 코사인 유사도 등)으로 계산했는지에 달렸다.


💡 Vector Similarity 실습 =>   
<https://github.com/heeyeonkoo99/Deep-learning_study/blob/master/%EC%8B%A4%EC%8A%B5/5_Vector%20Similarity.ipynb>


# 06. 토픽 모델링(Topic Modeling)  
토픽 모델링이란 기계 학습 및 자연어 처리 분야에서 토픽이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나로, 텍스트
본문의 숨겨진 의미구조를 발견하기 위해 사용되는 텍스트 마이닝 기법이다.  

### 1) 잠재 의미 분석(Latent Semantic Analysis, LSA)  
LSA는 정확히는 토픽 모델링을 위해 최적화 된 알고리즘은 아니지만, 토픽 모델링이라는 분야에 아이디어를 제공한 알고리즘이라고 볼 수 있다. 이에 토픽 모델링 알고리즘인 LDA에 앞서 배워보도록 하겠다. 뒤에서 배우게 되는 LDA는 LSA의 단점을 개선하여 탄생한 알고리즘으로 토픽 모델링에 보다 적합한 알고리즘이다.  
BoW에 기반한 DTM이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못한다는 단점이 있었다. (이를 토픽 모델링 관점에서는 단어의 토픽을 고려하지 못한다고도 한다.) 이를 위한 대안으로 DTM의 잠재된(Latent) 의미를 이끌어내는 방법으로 잠재 의미 분석(Latent Semantic Analysis, LSA)이라는 방법이 있다. 잠재 의미 분석(Latent Semantic Indexing, LSI)이라고 부르기도 한다.   
=> 특이값 분해(Singular Value Decomposition, SVD) : A가 m x n 행렬일때, 3개의 행렬의 곱으로 분해(decomposition)하는 것을 말한다.  
=> LSA의 경우 풀 SVD에서 나온 3개의 행렬에서 일부 벡터들을 삭제시킨 절단된 SVD(truncated SVD)를 사용한다. full SVD를 사용하는게 아님!  
=> 절단된 SVD는 대각 행렬의 대각 원소의 값중에서 상위값 t개만 남게 된다. 여기서 t를 선택하는 것은 쉽지 않은 일이다. t를 크게 잡으면 기존의 행렬
A로부터 다양한 의미를 가져갈 수 있지만, t를 작게 잡아야만 노이즈를 제거할 수 있기 때문이다. 이렇게 일부 벡터들을 삭제하는 것을 
데이터의 차원을 줄인다고도 말하는데, 데이터의 차원을 줄이게되면 당연히 풀 SVD를 했을때보다 직관적으로 계산비용이 낮아지는 효과를 얻을 수 있다.
여기서 계산비용이 낮아지는 것 외에도 상대적으로 중요하지 않은 정보를 삭제하는 효과를 가지고 있는데 이는 설명력이 높은 정보를 남긴다는 의미를
가지고 있다. 즉, 기존 행렬에서는 드러나지 않았던 심층적인 의미를 확인할수 있게 해준다.  

<img src = "https://user-images.githubusercontent.com/68431716/126419127-afae150e-7bbb-46db-a3bc-b6ef491f2b59.png" width="500px" height="300px">


### 2) 잠재 디리클레 할당(Latent Dirichlet Allocation, LDA)  
LDA는 문서의 집합으로부터 어떤 토픽이 존재하는지를 알아내기 위한 알고리즘이다. 
LDA는 앞서 배운 빈도수 기반의 표현 방법인 BoW의 행렬 DTM 또는 TF-IDF 행렬을 입력으로 하는데, 이로부터 알 수 있는 사실은 LDA는 단어의 순서는 신경쓰지 않겠다는 것이다.  
=> LSA : DTM을 차원 축소 하여 축소 차원에서 근접 단어들을 토픽으로 묶는다.  
=> LDA : 단어가 특정 토픽에 존재할 확률과 문서에 특정 토픽이 존재할 확률을 결합확률로 추정하여 토픽을 추출한다.

💡 Topic Modeling 실습 =>  
<https://github.com/heeyeonkoo99/Deep-learning_study/blob/master/%EC%8B%A4%EC%8A%B5/6_Topic%20Modeling.ipynb>


# 07. 머신 러닝(Machine Learning)개요     
하이퍼파라미터(초매개변수)는 모델의 성능에 영향을 주는 매개변수들을 말하고 보통 사용자가 직접 정해줄수 있는 변수를 말한다.
예를 들어 경사하강법에서 학습률(learning rate)이나 딥러닝에서 은닉층의 수, 뉴런의 수등을 말한다. 반면에 매개변수는 가중치와
편향과 같은 학습을 통해 바뀌어져가는 변수를 말하며 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값을 말한다.  

### 1) 개념 설명  
=> 지도 학습: 레이블(Label)이라는 정답과 함께 학습하는 것을 말한다.
=> 비지도 학습: 레이블이 없이 학습하는 것을 말한다. 토픽 모델링의 LDA가 이에 속한다.
=> 머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 한다. 하지만 정확도는 맞춘 결과와 틀린 결과에 대해
세부적인 내용을 알려주지 않는다. 예를 들어  
<img src = "https://user-images.githubusercontent.com/68431716/126624420-f24d75ab-46b7-4047-99c0-7489cf815848.png" width="200px" height="130px">  
일때   
<img src = "https://user-images.githubusercontent.com/68431716/126624693-249319bf-e0b0-41ee-bdc7-2d1b274afe07.png" width="200px" height="130px">
<img src = "https://user-images.githubusercontent.com/68431716/126624709-c4de89af-0574-425d-a159-30c67adec2c2.png" width="200px" height="130px">   
이렇게 표현할수 있다.   
=> 머신러닝에서 과적합(Overfitting)이나 과소적합(Underfitting)이되지 않고 적합(fitting)이 되도록 해야 한다.  


### 2) 선형 회귀(Linear Regression)    
변수 x의 값은 독립적으로 변할 수 있는 것에 반해, y값은 계속해서 x의 값에 의해서, 종속적으로 결정되므로 x를 독립 변수, y를 종속 변수라고도 한다. 선형 회귀는 한 개 이상의 독립 변수 x와 y의 선형 관계를 모델링한다. 만약, 독립 변수 가 1개라면 단순 선형 회귀라고 한다. 그게 아니라면 다중 선형 회귀라고 한다.  
=> 비용함수(Cost function): 평균 제곱 오차(MSE)  
=> 옵티마이저(Optimizer): 경사하강법(Gradient Descent)  

### 3) 로지스틱 회귀(Logistic Regression)    
=> 비용함수: 크로스 엔트로피 함수

### 4) 소프트맥스 회귀(Softmax Regression)    
앞서 로지스틱 회귀를 통해 2개의 선택지 중에서 1개를 고르는 이진 분류(Binary Classification)를 풀었다. 이번엔 3개 이상의 선택지중에서
1개를 고르는 다중 클래스 분류 문제를 위한 소프트맥스 회귀를 알아본다.  
=> 비용함수: 크로스 엔트로피 함수  
=> 오차 계산방법에 대해 소프트맥스 회귀에서는 실제값을 원-핫 벡터로 표현한다.  
<img src = "https://user-images.githubusercontent.com/68431716/126626333-a6c65c29-40f0-4e9b-945e-23cb868394b8.png" width="600px" height="200px">   

<img src = "https://user-images.githubusercontent.com/68431716/126626498-83db2c10-22b4-42f4-8724-4bf230c4fd5f.png" width="600px" height="130px">   

<img src = "https://user-images.githubusercontent.com/68431716/126626386-c8661e24-f305-49bb-b678-9a18b64426f0.png" width="600px" height="130px"> 
  
💡 Machine Learning 실습 =>  
<https://github.com/heeyeonkoo99/Deep-learning_study/blob/master/%EC%8B%A4%EC%8A%B5/7_Machine%20Learning.ipynb>


# 08. 딥 러닝(Deep Learning)개요  
딥 러닝의 기본 구조인 인공 신경망(Artificial Neural Network)의 역사는 생각보다 오래되었다. 딥 러닝을 보다 손쉽게 이해하기 위해서 1957년에 등장한 초기 인공 신경망부터 학습해보겠다. 이 챕터에서는 초기 신경망인 퍼셉트론(Perceptron)부터 피드 포워드 신경망 언어 모델의 정의, 그리고 기본적인 케라스의 사용법에 대해서 배워본다.  

### 1) 퍼셉트론(Perceptron)   
퍼셉트론(Perceptron)은 프랑크 로젠블라트(Frank Rosenblatt)가 1957년에 제안한 초기 형태의 인공 신경망으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘이다. 퍼셉트론은 실제 뇌를 구성하는 신경 세포 뉴런의 동작과 유사한데, 신경 세포 뉴런의 그림을 먼저 보도록 하겠다. 뉴런은 가지돌기에서 신호를 받아들이고, 이 신호가 일정치 이상의 크기를 가지면 축삭돌기를 통해서 신호를 전달한다.  
<img src = "https://user-images.githubusercontent.com/68431716/126858009-bb24a4de-44bc-4d51-90df-b4a1e4223718.png" width="600px" height="200px">
<img src = "https://user-images.githubusercontent.com/68431716/126858025-dc53a91e-eea8-432e-8376-c52e8b104a9b.png"  height="200px">  

신경 세포 뉴런의 입력 신호와 출력 신호가 퍼셉트론에서 각각 입력값과 출력값에 해당된다. x는 입력값을 의미하며 W는 가중치(Weight), y는 출력값이다.  
=> 단층 퍼셉트론(Single-Layer Perceptron) : 입력층(input layer)과 출력층(output layer)로 되어있다. 단층 퍼셉트론을 이용하면 AND,NAND, OR 게이트를
쉽게 구현할수 있다. 그러나 XOR 게이트는 구현이 불가능하다. 그 이유는 단층 퍼셉트론은 직선 하나로 두 영역을 나눌수 있는 문제에 대해서만 구현가능하기 때문이다. 이를 위해선
다층 퍼셉트론이 필요하다.  

<img src = "https://user-images.githubusercontent.com/68431716/126858141-0cc4ab3b-0654-4050-a304-ecf6c3976d81.png" width="350px" height="300px"><img src = "https://user-images.githubusercontent.com/68431716/126858164-1d6f17d2-c9eb-4d3c-b056-0b9837402fe3.png"  height="300px">  

=> 다층 퍼셉트론(MultiLayer Perceptron, MLP) : 입력층과 출력층 사이에 존재하는 층인 은닉층(hidden layer)이 있다. 다층 퍼셉트론은 본래 은닉층이 1개이상인 퍼셉트론을
말한다. 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 한다. 
<img src = "https://user-images.githubusercontent.com/68431716/126858223-662c495f-9737-4cee-9af3-b7afcade2e79.png" width="400px" height="300px">  

### 2) 인공 신경망(Artificial Neural Network) 훑어보기  
=> 피드 포워드 신경망(Feed-Forward Neural Network, FFNN)  
<img src = "https://user-images.githubusercontent.com/68431716/126858395-bf26f9a2-0ae0-4608-9abd-84576f00d381.png" width="400px" height="300px">
<img src = "https://user-images.githubusercontent.com/68431716/126858401-1fdbb489-6aa1-45e9-88b0-ffe97da6458c.png" width="400px" height="300px">   
RNN은 FFNN과 달리 은닉층의 출력값을 출력층으로도 값을 보내지만 동시에 은닉층의 입력으로 사용된다.  
=> 전결합층(Fully-connected layer, FC, Dense layer) : 어떤 층의 모든 뉴런이 이전 층의 모든 뉴런과 연결돼 있는 층을 말한다.  
=> 활성화 함수(Activation Function) : 비선형 함수(직선 1개로는 그릴수 없는 함수)여야 한다. 앞서 생각했던 계단함수도 마찬가지로 비선형 함수이며 활성화 함수다. 
인공신경망의 능력을 높이기 위해선 은닉층을 계속해서 추가해줘야 하는데 만약 선형함수로 사용하면 y(x)=kx와 같이 표현이 결국 되므로 은닉층을 여러번 추가해도
1번 추가한것과 차이를 줄수 없다.  
=> 행렬의 곱셈을 이용한 순전파(Forward Propagation)  

### 3) 딥러닝의 학습 방법  
=> 순전파  
=> 손실 함수 : MSE, Cross-Entroy등과 같은 함수가 있다.  
=> 옵티마이저 : 여기서 배치란 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양을 말한다. 옵티마이저로는 배치 경사 하강법/
확률적 경사 하강법/ 미니 배치 경사 하강법/ 모멘텀/ 아다그라드 /알엠에스프롭 /아담 등이 있다.
=> 역전파 : 인공신경망의 학습은 오차를 최소화하는 가중치를 찾는 목적으로 순전파와 역전파를 반복하는 것을 말한다.  
=> 에포크 : 인공 신경망에서 전체 데이터에 대해 순전파와 역전파가 끝난 상태를 말한다.  
=> 배치 크기 : 몇 개의 데이터 단위로 매개변수를 업데이트 하는지를 말한다. 기계 입장에서는 실제값과 예측값으로부터 오차를 
계산하고 옵티마이저가 매개변수를 업데이트한다. 이때 주의할점이 있다. 예를 들어 전체 데이터가 2000일때 배치 크기를 200으로 준다면
배치의 수(이터레이션)는 10이다.  
=> 이터레이션 : 한번의 에포크를 끝내기 위해서 필요한 배치의 수를 말한다.  

<img src = "https://user-images.githubusercontent.com/68431716/126859453-c6e82845-e3c2-4e40-ba8a-c72e94a42d99.png" width="400px" height="300px"> 

### 8) 케라스 서브클래싱 API   
케라스의 구현 방식에는 Sequential API, Functional API 외에도 Subclassing API라는 구현방식이 존재한다.  
=> Sequential API : 단순하게 층을 쌓는방식으로 쉽지만 다수의 입출력에는 어렵다.  
=> Functional API : Sequential API로는 구현하기 어려운 복잡한 모델들을 구현할수 있지만 입력의 크기를 명시한 입력층을 모델의 앞단에 정의해야 한다.  
=> Subclassing API : Functional API로도 구현할수 없는 모델들조차 구현가능하지만 코드사용이 가장 까다롭다. 객체지향프로그래밍에 익숙해야한다.

<img src = "https://user-images.githubusercontent.com/68431716/126860409-7ad4834f-b067-47aa-8fdd-8004782c26fa.png" width="700px" height="300px">   


💡 Deep Learning 실습 =>  
<https://github.com/heeyeonkoo99/Deep-learning_study/blob/master/%EC%8B%A4%EC%8A%B5/8_Deep%20Learning.ipynb>

 




















[je>kyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/

